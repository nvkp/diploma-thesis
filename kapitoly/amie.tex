\chapter{AMIE Algorithm and Its Derivatives\label{amieandits}}

Finding association rules in knowledge bases can serve several purposes. New facts that are not yet present in the dataset can be derived from the found regularities described by the rules. From such rules opposing facts present in the dataset can be deduced to be wrong. Mined rules can also help to understand the data better.

For mining rules from a graph database such as LOD datasets, Inductive Logic Programming (ILP) can be used. ILP works under the Closed World Assumtion (CWA) meaning it supposed that both negative and positive statements are present in the data. However LOD operates under the Open World Assumption (OWA) ie. if a statement is not present in the data, it does not mean that this statement does not correspond to reality. Rules mined by ILP would not reflect this matter. Moreover ILP are not observed to be efficient over large datasets in the order of millions of statements, making it not a viable way to mine rules over real-world knowledge bases such as YAGO or Wikidata.

\section{AMIE\label{amie}}

An algorithm that is specifically designed to mine rules from data operating under OWA and constisting of binary predicates (just as Linked Data) is AMIE.\cite{Galarraga2013} AMIE mines rules in the form of Horn rule. Horn rule is an implication with conjunction of atoms on the left side, called body and a single atom on right side call head. We can imagine an atom as an RDF triple, where subject and object can be replaced by variables. Number of atoms in a rule indicates the length of the rule. In this work rules are represented with an infix notation. The AMIE literature use the Datalog notation commonly used in ILP domain. An example of a rule AMIE seeks to discover is shown below. 

\label{asocRule}
$$
(?a\hspace{0.4em}worksIn\hspace{0.4em}?b) \land
(?b\hspace{0.4em}hasHeadquatersIn\hspace{0.4em}?c) \Rightarrow
(?a\hspace{0.4em}livesIn\hspace{0.4em}?c)
$$

This rule states that any person lives in a place his or her company's headquaters. Length of this rule is 3 since it has two body atoms. The rule has 3 variables. When we substitute the variables by constants present in the examined data set, we get an \textit{instantiation} of the rule. If all atoms of the instantiated rule appear in the data set, the head atom of the instantiation is one the \textit{predictions} of the rule. Number of all instantiations of an atom that appear in the data set is called \textit{size} of the atom.

\subsection{Language Bias\label{languagebias}}

In order to efficiently traverse the search space, AMIE subjects the rules to a particular language bias. Only the rules comforming the conditions stated below can be generated and further refined.

\begin{description}
    \item[rules have to be connected] A rule is connected when every atom in the rule shares every variable with another atom in the rule.
    \item[rules have to be closed] A rule is closed when every variable appears at least twice in the rule.
    \item[rules cannot be reflexive] reflexive rule contains at least one atom with identical subject and object variable or constant.
    \item[rule can be recursive] Any predicate can occur more then once in a rule.
\end{description}

\subsection{Measures of Significance}

\subsubsection{Support}

For a chosen definition of a support measure for the AMIE algorithm it is crucial for the definition to have the property of monotonicity ie. by adding any new atom to the body of a rule, the support of the rule shall always decrease or remain the same. A naive way to count support of a rule would be to count all instantiations of the rule that appear in KB. Such definition would not comply the property of monotonicity, since an addition of a dangling atom to a rule would introduce a new variable multiplying the number of instantiations and thus the value of the support measure. By counting only all distinct pairs of subjects and objects in the head of all instantiations that appear in KB, the property of monotonicity is preserved:

$$ supp(\vec{B} \Rightarrow r(x,y)) := \# (x,y): \exists z_{1}\ldots z_{n}: \vec{B} \land r(x,y) $$

\subsubsection{Head Coverage}

Since the support is an absolute number, so the size of the examined data set has to be taken into account while defining this threshold. Plus If the defined support value is greater than a number of distint triples containing a certain predicate, any rule containing this predicate in the head atom would be disregarded. Head Coverage is the relative expression of support. It is defined as support of a rule over the numbers of triples with the head's predicate, so-called \textit{head size}.

$$ hc(\vec{B} \Rightarrow r(x,y)) := \frac{supp(\vec{B} \Rightarrow r(x,y))}{size(r)}  $$

\subsection{Confidence Measures}

The above-mentioned measures describe a quantitative significance of the rule in relation to the examined data set. They quantify the true predictions of the rule but do not take into account the false predictions. Confidence is a way to measure the quality of a rule. Generally speaking, confidence is a ratio of true predictions of a rule to the sum of true predictions and the counterexamples. Number of true predictions can easily be expressed by the rule's support. Two different ways to count the counterexamples are discussed below.

\subsubsection{CWA and Standard Confidence}

Standard confidence considers every fact that is not present in the examined dataset a false fact and thus a counterexample when predicted by a rule. A fact predicted by a rule is either present in the data set or it is not. Therefore the standard confidence is defined as the ratio of the number of true predictions of the rule to the number of all predictions of the rule.

$$ conf(\vec{B} \Rightarrow r(x,y)) := \frac{supp(\vec{B} \Rightarrow r(x,y))}{\# (x,y): \exists z_{1}\ldots z_{n}: \vec{B}} $$

This way of generating counterexamples fails to distinguish a false fact from an unknown fact. This conforms the CWA and it is traditionally used for association rule mining over transactional data where this assumption can be applied. For example if the data does not state, that I bought a bottle of milk last Wednesday, then I really did not buy it. AMIE, however, is intended to mine rules from data operating under OWA, so the usage of this measure is inappropriate.

\subsubsection{PCA Confidence}

For the PCA Confidence, Partial Completeness Assumption (PCA) is used for generating the counterexamples:

If $\langle s\hspace{0.4em}p\hspace{0.4em}o\rangle\in KBtrue$ then $\forall_{o'}: \langle s\hspace{0.4em}p\hspace{0.4em}o'\rangle))\in(KBtrue \cup NEWtrue) \Rightarrow \langle s\hspace{0.4em}p\hspace{0.4em}o'\rangle\in KBtrue$.

Meaning that if we know any object for given predicate and subject, we know all triples of containing the predicate and subject together. This assumption is certainly true for predicates with high or complete functionality, such as birthdate or capital. A triple predicted by the measured rule is considered an counterexample only when triples with its combination of subject and predicate are present in the data set and none of those has the triple's object. 

% todo priklad jak se obe spocitaji

% The assumption is also true in the vast majority of cases for relations that are not functional, but that have a high functionality.

% we normalize the confidence not by the entire set of facts, but by the set of facts of which we know that they are true, together with the facts of which we assume that they are false.

$$conf_{pca} := \frac{supp(\vec{B} \Rightarrow r(x,y))}{\# (x,y): \exists z_{1}\ldots z_{n},y': \vec{B}\land r(x,y')}$$

\subsection{Algorithm}

The AMIE algorithm takes as input parameters the RDF graph the rules are to be mined from (\textit{KG}), the minimum head coverage of the rules (\textit{minHC}), maximal length of the rules (\textit{maxLen}) and the minimal confidence of the rules (\textit{minConf}). For each distinct predicate in the graph a rule with empty body and a head atom with this predicate is created an the algorithm is initialized by filling a queue with those rules. 

The algorithm than iteratively dequeues rules from the queue. If the dequeued rules complies the criteria set by the input parameters, it is accepted for output and added to the result rules array. If rule's length is shorter than the maximal stated length, the rule is refined meaning new atoms are added to its body to create new rules. If a new rule reaches the defined minimal head coverage and is not already present in the queue, it is added to the rule queue. The algorithm continues dequeueing the rules until the queue is empty and no more new rules can be created by the refinement.

\begin{algorithm}
\caption{AMIE algorithm}
\scriptsize
\begin{algorithmic}[1]
\Procedure{AMIE}{$KG, minHC, maxLen, minConf$}
\State $queue = [(?a\hspace{0.4em}r_{1}\hspace{0.4em}?b), (?a\hspace{0.4em}r_{2}\hspace{0.4em}?b) \ldots (?a\hspace{0.4em}r_{m}\hspace{0.4em}?b)]$
\State $output = \langle \rangle$
\While {$\neg queue.isEmpty()$}
\State $rule = queue.dequeue()$
\If {$AcceptedForOutput(r, out, minConf)$}
\State $output.add(rule)$
\EndIf
\If {$length(rule) < maxLen$}
\State $R(rule) = Refine(rule)$
\EndIf
\For {$r_{i}\in R(rule)$}
\If {$hc(r_{i} \geq minHC\hspace{0.4em}\&\hspace{0.4em}r_{i} \notin queue)$}
\State $queue.enqueue(r_{i})$
\EndIf
\EndFor
\EndWhile
\State \Return $output$
\EndProcedure
\end{algorithmic}
\end{algorithm}

When Choosing larger thresholds on minimal head coverage and minimal confidence and shorter maximum length makes the algorithm stop earlier and generate fewer rules. Choosing smaller thresholds and allowing generation of longer rules results in longer runtime and more found rules. So the setting of these parameters always introduces a trade-off between the runtime and the number of returned rules.

\subsection{Rule Refinement}

By simply enumerating all possible rules and then computing the interest measures for them in order to find the significant ones is not a fiasible approach for large graphs. The exploration of the the search space of the rules has to be done efficiently. Such exploration is performed by the rules refinement. During the rule refinement, the new atoms are added to the rule by three means (called \textit{operators}). Each operator creates zero to many rules when applied to a rule.

\begin{description}
    \item[$O_{D}$] A \textit{dangling} atom is added to the rule. This atom introduces a new variable to the rule. Its second variable is already present in the rule.
    \item[$O_{I}$] An \textit{instantiated} atom extends the rule. Instantiated atom contains a constant and a variable already present in the rule.
    \item[$O_{C}$] A \textit{closing} atom is added. This atom's both variables are already present in the rule.
\end{description}

\subsection{Querying the Graph}

The algorithm uses so-called \textit{count projection queries} to find the predicates and entities with which new atoms are created during the rule refinement by applying the mining operators, such that the minimum head coverage of the new rule is reached. These queries are not efficient when implemented in SPARQL or SQL. So the authors of the AMIE algorithm suggested in-memory database that is tailored to this type of queries.

The data structure constists of six \textit{fact indexes}: each for a permutation of subject (\verb|S|), predicate (\verb|P|) and object (\verb|O|). Let they be denoted as \verb|SPO|, \verb|SOP|, \verb|PSO|, \verb|POS|, \verb|OSP| and \verb|OPS|. Each fact index is a hash table that maps elements of the first column to a hash table that constains elements of the second column as key mapping to a set of third column elements, such that triples in the graph containing the first column element, second column element and the third column element exist. For example the index SPO is a hash table with each subject present in the graph as a key. Every subject key points to a hash table with keys of predicates that exist with this subject in at least one triple in the graph. The predicate key then points to a array of objects for which triples with this subject, predicate and object exist in the graph. That means that each triple in the graph is store six times in this database.

Along with the fact indexes, the \textit{aggregated} indexes are used for the algorithm. There are three (\verb|S|, \verb|P|, \verb|O|) of them for each triple position and they store number of triples in the graph that contain the element of corresponding key. For example the index \verb|P| stores number of triples for each distinct predicate in the graph. The numbers have to be precomputed before initializing the mining. With these indexes, it is easy to check of size of an atom (\textit{size queries}). When computing for example the size of atom \verb|?a knows ?b| the algorithm would turn to the aggregated index P and simply look up the number corresponding to the key \verb|knows|. For an atom with a constant instead of one of the variables, a fact index is used. For computing the size for an atom \verb|?a knows John_Smith| the algorithm would look into the \verb|POS| index and find a hash table corresponding to the predicate \verb|knows|. Then in this hash table it would find the array corresponding to the key of the entity \verb|John_Smith| and count the items in the array. For checking the existence of a triple any fact index can be used. Either way the database allows checking for existence of a triple or computing atom's size in constant time. A drawback of this is that the database is six times memory-demanding than the input graph itself.

The algorithm exercices other types of queries. The size queries are part of the so-called \textit{existence queries} that check for existence of a binding of a conjunction of atoms. The existence queries are in turn used in the so-called \textit{select queries} that look for all instances of a variable in a conjunction of atoms. The select queries are part of the \textit{count queries} that count distinct bindings to variables of an atom in a conjunction of atom. This is useful for computing the confidence of a rule. 

For the PCA confidence of a rule the denominator is the bindings count of head atom's variables $x$ and $y$ for which $(x\hspace{0.4em}r\hspace{0.4em}y') \land \vec{B}$ can be instantiated from the graph. To compute the denominator, the algorithm first fires SELECT DISTINCT $x$ where $(x\hspace{0.4em}r\hspace{0.4em}y') \land \vec{B}$. Then for each found $x$ it instantiates the conjunction with with $x$ and fires SELECT DISTINCT $y$ where $(x\hspace{0.4em}r\hspace{0.4em}y') \land \vec{B}$. By adding up the numbers of distinct $y$ from each query the denominator is computed.

\subsubsection{Count Projection Queries}

The select queries are also necessary for the count projection queries to find the predicates and entities for new atoms for a rule during the rule refinement. The algorithm considers the set minimal head coverage when creating the new atoms to be added. Only the atoms are added for which the new rule still reaches the minimal head coverage. The general structure of a count projection query is as this:

$SELECT$ $x,$ $COUNT(H)$ $WHERE$ $H \land \vec{B}\\$
$SUCH$ $THAT$ $COUNT(H)\geq k$

where x can represent either predicates or entities of new atoms. $\vec{B}$ is body of the new rule including the added atom. The symbol $k$ stands for an absolute translation of the minimal head coverage for the rule: $minHC \times size(H)$. The authors of the algorithm provide following example for explaining the usage of count projection queries during a rule refinement. The rule $(?x\hspace{0.4em}marriedTo\hspace{0.4em}?z) \Rightarrow (?x\hspace{0.4em}livesIn\hspace{0.4em}?y)$ is about to be refined. When applying a dangling atom operator to this rule, atoms are to be created that contain a new variable ($?w$). The new variable in the new atoms will be either:

\begin{itemize}
    \item at the position of subject and the variable $?y$ will be at the position of object: $(?w\hspace{0.4em}p\hspace{0.4em}?y)$
    \item at the position of subject and the variable $?z$ will be at the position of object: $(?w\hspace{0.4em}p\hspace{0.4em}?z)$
    \item at the position of object and the variable $?y$ will be at the position of subject: $(?y\hspace{0.4em}p\hspace{0.4em}?w)$
    \item at the position of object and the variable $?z$ will be at the position of subject: $(?z\hspace{0.4em}p\hspace{0.4em}?w)$
\end{itemize}

For each of those four \textit{join combinations} a count projection query will be fired to find predicates to be substituted into them:

$SELECT$ $p,$ $COUNT(?x\hspace{0.4em}livesIn\hspace{0.4em}?y)\\$ 
$WHERE$ $(?x\hspace{0.4em}livesIn\hspace{0.4em}?y) \land (?x\hspace{0.4em}marriedTo\hspace{0.4em}?z) \land (X\hspace{0.4em}p\hspace{0.4em}Y)\\$
$SUCH$ $THAT$ $COUNT(?x\hspace{0.4em}livesIn\hspace{0.4em}?y)\geq k$

$(X\hspace{0.4em}p\hspace{0.4em}Y) \in \{(?w\hspace{0.4em}p\hspace{0.4em}?y),(?w\hspace{0.4em}p\hspace{0.4em}?z),(?y\hspace{0.4em}p\hspace{0.4em}?w),(?z\hspace{0.4em}p\hspace{0.4em}?w)\}$

If the dangling atom operator is applied on an intermediate rule (not all variables are closed) like this one, the dangling atoms are joined on the non-closed variables ($?y$ and $?z$) in this case. If the rule is closed, the dangling atom is joined on all variables in the rule. The closed atom operator would apply join combinations of $(?y\hspace{0.4em}p\hspace{0.4em}?z)$ and $(?z\hspace{0.4em}p\hspace{0.4em}?y)$ for this example. If there were only one open variable, the closed atom operator would apply join combinations with this variable and each other variable in the rule. If there were no open variable, the operator would apply join combinations of all possible pairs of variables in the rule. When applying the instantiated atom operator, first the dangling atom is used to generate new atoms with new variable. For each this new atom a count projection query on instance of the new variable in the atom is fired to find all entities, that can substitute the variable and the minimal head coverage is still reached. Each found entity then forms a new rule with the new atom with the instantiated variable. This is an example of the query for atom $(?x\hspace{0.4em}citizenOf\hspace{0.4em}?w)$:

$SELECT$ $w,$ $COUNT(?x\hspace{0.4em}livesIn\hspace{0.4em}?y)$ $WHERE\\$
$(?x\hspace{0.4em}livesIn\hspace{0.4em}?y) \land (?x\hspace{0.4em}marriedTo\hspace{0.4em}?z) \land (?x\hspace{0.4em}citizenOf\hspace{0.4em}w)\\$
$SUCH$ $THAT$ $COUNT(?x\hspace{0.4em}livesIn\hspace{0.4em}?y)\geq k$

\section{AMIE+}

In \cite{Galarraga2015} the authors of the AMIE algorithm came up with an improved version reffering to it as AMIE+. The improvements lie in the rule refinement phase and the confidence evaluation. The improvements do not alter the output of the algorithm, they only speed it up so that it is applicable to mining over large knowledge bases.

\subsection{Rule Refinement}

From the refined rule, it is sometimes possible to infere that an application of a operator would not yield better on any new rules. AMIE+ adds these checks before an operator application:

\begin{enumerate}
    \item When a not-closed rule with a length of $maxLen - 1$ is being refined, the dangling atom operator is not applied, since the new variable cannot be closed before exceeding the $maxLen$ threshold.
    \item When a not-closed rule with a length of $maxLen - 1$ and more than two open variables is being refined, the closed atom operator is not applied, because one variable would still be open when the rule exceeds the $maxLen$ constraints so no such rules would be accepted for output.
    \item A not-closed rule with a length of $maxLen - 1$ and more than one open variable is not applied an instantiated atom operator on, because the instantiated atom operator can only close at most one variable.
    \item If the refined rule reaches PCA confidence of 1, it is no further refined, since the new rule would not increase in confidence. They can only decrease in support.
\end{enumerate}

Also in some cases a dangling atom cannot reduce support of a rule. It is when the parent rule already contains atoms with the same relation as a dangling atom and these atoms have a variable in common with the dangling atom. The child rules would have the same support as the parent rule, so the support computation for these rules can be skipped.

\subsection{Speeding Up Confidence Evaluation}

The most important improvement is the confidence approximation. Authors state that during experiments with AMIE, 35\% of runtime was spent on the confidence compututation. The algorithm has to at first compute the rule's confidence and only after that the rule can be discarded when it does not the reach a $minConf$ threshold. So the algorithm can spend hundreds of milliseconds computing confidence of a rule for nothing. But instead of computing it, the confidence can be estimated, which is much faster (the authors claim 200-fold speed up). The approximation is based on statistics about predicates (functionality and inverse functionality of predicates, size of the joins betweeen predicates etc.) in the input graph, that are precomputed when the input graph is loaded into the in-memory database. 

The approximation is designed to overestimate the confidence so that no high confidence rules are pruned. But the approximation does not just simply substitute the exact confidence computation. The approximation is only applied to rules that have intermediate variables (variables that do not appear in the head atom) and there exists a single path to one head variable to the other through the intermediate variables. For example the rule $(?a\hspace{0.4em}livesIn\hspace{0.4em}?b) \land (?b\hspace{0.4em}hasStreet\hspace{0.4em}?c) \Rightarrow (?a\hspace{0.4em}worksIn\hspace{0.4em}?c)$ has one intermediate variable $?b$. There is a single path of variables connecting the variables of the rule's head: $?a \rightarrow ?b \rightarrow ?c$. So this rule's confidence would be approximated instead of computed. Rules without intermediate variables are supposed to have smaller number of bindings to their body so the confidence should be quicker to computed. Confidence for ules with more than single path connecting the head variables (such as $(?a\hspace{0.4em}livesIn\hspace{0.4em}?b) \land (?b\hspace{0.4em}hasStreet\hspace{0.4em}?c) \land (?a\hspace{0.4em}bornIn\hspace{0.4em}?d) \land (?d\hspace{0.4em}hasStreet\hspace{0.4em}?c)\Rightarrow (?a\hspace{0.4em}worksIn\hspace{0.4em}?c)$ that has two paths: $?a \rightarrow ?b \rightarrow ?c$ and $?a \rightarrow ?d \rightarrow ?c$) is also thought to be easy to compute because more variable paths tend to restrict the number of head variables bindings. If the estimated value reaches the $minConf$ threshold, the exact confidence value is computed as well.

\section{RDFRules}

Although the AMIE+ algorithm made it possible to mine rules from large knowledge bases, its use would have to be combined with other tools when deployed on real-world knowledge bases, since it does not deal with preprocessing the input data (e.g. unifying IRIs for identical resources, treatment of numerical values) and subsequent analysis of the generated rules. An extension of the AMIE+ algorithm under the name of \textit{RDFRules} was presented in \cite{Zeman2020}. Beside enhancements to the algorithm itself (faster projection counting pruning by used specified \textit{rule patterns}), several other techniques were proposed to be integrated with the algorithm to cover complete mining process.

% \subsection{Faster Projection Counting}

% TODO

\subsection{Processing of Numerical Attributes\label{numericalattributes}}

AMIE+ treats numerical values (in RDF those can only appear at the position of object) just as any other distrete value. Due to the downward closure property of the algorithm (and of the association rule mining algorithms in general), where not only the whole rule but also each subset of the rule's atom have to meet the minimum support threshold, and due to the fact that the numerical attributes usually contain many distinct values, the rules concerning numerical values tend to be excluded from the generated rules.

This problem can be solved by the values' discretization meaning that the continuous values are converted to a finite set of intervals. When working with transactional data such sets of intervals are created within each examined table's column which contains numerical values. For example the \textit{arules}\footnote{\href{https://www.rdocumentation.org/packages/arules/versions/1.6-8}{https://www.rdocumentation.org/packages/arules/versions/1.6-8}} R package implementing the Apriori algorithm enables discretizing\footnote{\href{https://www.rdocumentation.org/packages/arules/versions/1.6-8/topics/discretize}{https://www.rdocumentation.org/packages/arules/versions/1.6-8/topics/discretize}} the values either equidistantly (the intervals have a given fixed length), equifrequently (a given number of intervals are evenly represented in the continuous data points), by clustering the values to a given number of intervals or by stating the intervals manually.

In the context of RDF the discretization entails grouping numerical values in the range of a certain predicate. This can also be done equifrequently, equidistantly etc. However, when the created intervals are too narrow, the rules containing the discretized predicates may not satisfy the minimum support or the head coverage threshold defined for the mining task. When creating too broad intervals the found rules may be unnecessarily general. That means that the discretization phase cannot easily be decoupled from the mining phase itself.

In \cite{Zeman2020} the authors propose a discretization heuristic that takes into account the minimum head coverage and minimum head size thresholds to perform a specific discretization for a particular task in the pre-processing phase. For each predicate in the data set that has a numerical range a set of overlapping intervals is generated. For each interval in this set and for each predicate's triple with the number in the range of the interval a copy of the triple whose object is substituted and whose predicate is altered by a suffix of the interval set (not to modify the range of the original predicate) by the interval is added to the data set.   

\subsection{Multiple Graphs}

Restricting the mining task on to a single graph prevents finding relations accross multiple sources. Even though the resources representing the identical object or a concept in the real world have a different identifier in different graphs, the identity can be easily infered from the \verb|owl:sameAs| predicate joining two identical resources.

$$(?a \hspace{0.4em} \langle wasBornIn \rangle \hspace{0.4em} ?b \hspace{0.4em} \langle YAGO \rangle) \Rightarrow (?a \hspace{0.4em} dbo:deatchPlace \hspace{0.4em} ?b \hspace{0.4em} \langle DBpedia \rangle)$$

AMIE+ does not recognize the \verb|owl:sameAs| statements and assumes that the identical resources have the same IRI. To mine from accross multiple graphs the graph would either have to be merged together and their IRIs would have to be unified or the \verb|owl:sameAs| relations would have to be explicitely stated in the rules.

RDFRules natively supports \textit{quads} i.e triples enriched with IRI of their corresponding named graph. On top of the AMIE's six indeces four new ones are added (\textbf{PG}, \textbf{PSG}, \textbf{POG}, \textbf{PSOG}) that allow to check the affiliation of triples to a graph. Not only the individual triples but also the atoms in a rule can be treated as quads. The atoms can be restricted via the rule patterns (see \ref{patterns}) to be based only on triples from a certain graph. The RDFRules algorithm recognizes the \verb|owl:sameAs| triples and the resources joined this way are treated as if they had identical identifiers.

\subsection{Improvements to Expressivenes of Rule Patterns\label{patterns}}

The association rules mining algorithms tend to create rules describing obvious and uninteresting patterns when the search space of the rules is not properly restricted. AMIE allows the user to provide a list of predicates that should be included or excluded in the rules' body and head and also to prohibit or enforce contants, so that the shape of the rules can be controlled to some extent. RDFRules offers much more profound solution in the form of a rule pattern grammar. 

The user defines rule patterns which are used to prune the rules during the rule refinement. Each rule has to match at least one of the rule patterns defined for the mining task. A rule pattern consists of atom patterns corresponding to atoms of a matching rule. The rule pattern can have zero or any number of atom pattern in the body and exactly one atom pattern in its head. A rule pattern with atom patterns in the body and no atom pattern in the head is useless since the rule patterns are applied from right to left just as the rule refinement operators. An atom pattern consists of four atom item patterns that correspond to subject, predicate, object and graph of the matching rule atom. Those atom item patterns are available:

\begin{description}
\item[$?$] pattern for any symbol at the position
\item[$?_{v}$] pattern for any variable
\item[$?_{c}$] pattern for any constant
\item[$?a$] pattern for a concrete variable written as a single alphabetic character after the symbol $?$ 
\item[[]] collection of items where at least one has to be matched at the position, that can be also negated by the symbol $\neg$ stating that none of the items must match at the position
\end{description}

Shown below is an example of a valid rule pattern and its matching rule:

$$(?a\hspace{0.4em} rdf:type \hspace{0.4em}?_{c}) \land (?a\hspace{0.4em} ? \hspace{0.4em}?_{v}) \Rightarrow (?a\hspace{0.4em}? \hspace{0.4em}?)$$

$$(?a\hspace{0.4em} rdf:type \hspace{0.4em}dbo:Scientist) \land (?a\hspace{0.4em} dbo:academicDiscipline \hspace{0.4em}?b) \Rightarrow (?a\hspace{0.4em}dbo:knownFor \hspace{0.4em}?b)$$

\subsection{Top-k Approach}

RDFRules algorithm offers an alternative to manually defining an interest measure (e.g. minimum support or minimum confidence) for the mining task. Instead, in the so-called \textit{top-k approach}, the user defines a maximum number of rules with the highest values of a chosen measure that should be returned by the algorithm. During the mining process, the rules are stored and sorted in a queue with a fixed length of the defined number. The head of the queue  contains a rule with the lowest value of the chosen measure. This value acts as a temporary minimum threshold. Once a rule with a higher value of the measure is found, the head rule is removed from the queue, the new rule is added to the queue and the queue is sorted again so that the head of the queue still contains the rule with the lowest value. At the end of the mining process only the rules in the queue are returned. 

\subsection{Support for the Lift Measure}

The lift is a measure that describes how the probability of the consequent (head) occurrence is increased given that the antecedent (body) of the rule is valid compared to its probability of occurence under a random choice in the complete dataset. RDFRules adds support for this measure. It is computed as a ratio of the rule's confidence and its \textit{head confidence}.

$$lift(\vec{B} \Rightarrow H) = \frac{conf(\vec{B} \Rightarrow H)}{hconf(H)}$$

The formula of the head confidence depends on the type of the head atom. If the head atom contains two variables ($H = (?a\hspace{0.4em}p\hspace{0.4em}?b)$), the head confidence is computed as the ratio between the number of distinct subjects bound with the predicate in the head atom and the number of distinct subjects in the whole data set.

$$hconf(\vec{B} \Rightarrow H) = \frac{\# s: \exists \langle s,p,o\rangle \prec (?a\hspace{0.4em}p\hspace{0.4em}?b) }{\# s: \exists \langle s,p,o\rangle \prec (?a\hspace{0.4em}?r\hspace{0.4em}?b) }$$

If the head atom contains one variable and a constant at the position of object ($H = (?a\hspace{0.4em}p\hspace{0.4em}C)$), then the head confidence is computed as the ratio between the number of distinct subjects bound with the predicate in the head atom and with the constant in the head atom and the number of distinct subjects in the whole data set.

$$hconf(\vec{B} \Rightarrow H) = \frac{\# s: \exists \langle s,p,o\rangle \prec (?a\hspace{0.4em}p\hspace{0.4em}C) }{\# s: \exists \langle s,p,o\rangle \prec (?a\hspace{0.4em}p\hspace{0.4em}?b) }$$\label{hconf2}

If the head atom contains a constant at the position of subject ($H = (C\hspace{0.4em}p\hspace{0.4em}?a)$), the formula goes as the ratio of between the number of distinct objects bound with the predicate in the head atom and with the constant in the head atom and the number of distinct subjects in the whole data set.

$$hconf(\vec{B} \Rightarrow H) = \frac{\# s: \exists \langle s,p,o\rangle \prec (C\hspace{0.4em}p\hspace{0.4em}?a) }{\# s: \exists \langle s,p,o\rangle \prec (?a\hspace{0.4em}p\hspace{0.4em}?b) }$$

\subsection{Rule Clustering and Pruning}

When an association rule mining algorithm generates a high number of overlapping rules, a clustering algorithm can be used to group the rules, so that the rules can be presented in a more compact and organized way e.g. as a result of an exploratory analysis. In \cite{Zeman2020} propose an approach to determine the similarity of two rules based on their content and their interest measures. When comparing the content similarity of rules $U$ and $V$, where $|U| \geq |V|$ the similarity is computed as the maximum average of atom similarities between atoms of the rule $V$ and a $k$-permutation of the atoms from the rule $U$ where $k = |V|$.

$$sim_{c}(U,V) = \dfrac{1}{|V|} \max\limits_{T\in P(U,|V|)}\sum_{i = 1}^{|T|} sim_{a}(t_{i},v_{i})$$

The similarity of two atoms is based on the similarities between their subjects, predicates and objects.

$$sim_a(A_{1}, A_{2}) = \frac{1}{3} [ \, sim(\langle s_{1}, p_{1} \rangle, \langle s_{2}, p_{2} \rangle) + sim(\langle o_{1}, p_{1} \rangle, \langle o_{2}, p_{2} \rangle) + sim(p_{1}, p_{2}) ] \,$$

The similarity function comparing two predicates returns the value $1$ if the predicates are identical and $0$ otherwise. The similarity function comparing two subjects (and objects analogously) returns the value $1$ either if the subjects are identical and they are not variables or if they are both variables and the predicates of the two rules are identical, it returns the value $0.5$ if the subjects are not identical, however the predicates are identical and exactly of the the subjects is a variable, and it returns $0$ otherwise.

To avoid a situation where a single triple in the data set is covered by multiple rules returned by the algorithm, the authors of RDFRules suggest adapting the \textit{data coverage pruning} in the post-processing phase. The rules are ranked based on the following criteria:

\begin{enumerate}
    \item $conf(A) > conf(B)$
    \item $conf(A) = conf(B)$ and $hc(A) > hc(B)$
    \item rule $A$ has a shorter body than the rule $B$
\end{enumerate}

Then for each rule starting with the highest ranked to the lowest ranked rule it is checked whether the rule covers at least one triple (i.e. any atom in the rule can be instantiated by the triple) that the previous rules did not and only those rule which satisfy the conditions are kept. That way the new set of rules covers exactly the same set of triples in the data set.